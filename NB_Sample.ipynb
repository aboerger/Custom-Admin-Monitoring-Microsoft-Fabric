{"cells":[{"cell_type":"markdown","source":["### Using Semantic Link Labs to run Best Practice Analyzer\n","\n","https://github.com/microsoft/semantic-link-labs\n","\n","https://semantic-link-labs.readthedocs.io/en/latest/"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"d6e4e867-6659-4035-900f-f5b419a1df02"},{"cell_type":"code","source":["import sempy\n","import sempy.fabric as fabric\n","import sempy_labs as labs"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"129a950e-a78e-4ec2-a4c4-d98aff6ff935"},{"cell_type":"code","source":["# Run BPA for a single model, displaying the results in an HTML table within the notebook\n","\n","bpa_results = labs.run_model_bpa(\n","    dataset = \"My Semantic Model\",\n","    workspace = \"My Workspace\"\n","    )"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"b5d43f67-085b-4197-bf4a-0595de233816"},{"cell_type":"code","source":["# Obtain a list of all workspaces in a given Fabric capacity\n","\n","capacityID = 'My capacity GUID' \n","x = fabric.list_workspaces()\n","filter_condition = [capacityID]\n","x = x[x['Capacity Id'].isin(filter_condition)]\n","workspace_names = x['Name'].tolist()"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"64e9d24e-5374-47b1-be19-1d5fb16fe74d"},{"cell_type":"code","source":["# For the list of workspaces, run BPA against all the semantic models within. Specify models to be excluded from scanning\n","# The results will be saved as a Delta table within the default lakehouse attached to the notebook\n","\n","labs.run_model_bpa_bulk(\n","    workspace=workspace_names, \n","    skip_models=\n","        [\n","            'ModelBPA', \n","            'Fabric Capacity Metrics', \n","            'Report Usage Metrics Model', \n","            'Usage Metrics Report', \n","            'Dashboard Usage Metrics Model'\n","        ]\n","    )"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"b7335055-04c9-467c-9e09-db239063e31c"},{"cell_type":"code","source":["# Create a direct lake semantic model using the table we created above\n","\n","labs.create_model_bpa_semantic_model()"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"83c479b0-5e30-4536-b5a1-1cbe097d63eb"},{"cell_type":"code","source":["# Create a default report using the model we created above\n","\n","import sempy_labs.report as report\n","report.create_model_bpa_report()"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"eeaad941-3436-4dc5-b890-0412c1a82db4"},{"cell_type":"markdown","source":["### Get Activity Events to retain audit logs from your Fabric/PBI tenant\n","The below sample code connects to the Activity Events endpoing and saves audit logs \n","\n","https://learn.microsoft.com/en-us/rest/api/power-bi/admin/get-activity-events"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"69a1e5db-c15e-4a4e-88cd-bd66eb1d3e54"},{"cell_type":"code","source":["# Initial imports and configuration\n","\n","spark.conf.set(\"sprk.sql.parquet.vorder.enabled\", \"true\")\n","spark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\n","spark.conf.set(\"spark.microsoft.delta.optimizeWrite.binSize\", \"1073741824\")\n","\n","import json\n","import logging\n","import requests\n","import msal\n","import pandas as pd\n","from datetime import date, timedelta\n","from delta.tables import *\n","from pyspark.sql.functions import col, year, month, dayofmonth\n","\n","workspace_id = spark.conf.get(\"trident.workspace.id\")\n","lakehouse_bronze_id = notebookutils.lakehouse.get(\"Your Lakehouse Name\", workspace_id)['id']"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"fd5fd2d0-2b59-4900-8bad-84ea2f3e8ed2"},{"cell_type":"code","source":["# Authenticate using a service principal\n","\n","config = {\n","            \"authority\": \"https://login.microsoftonline.com/<YOUR TENANT ID>\",\n","            \"client_id\": \"<YOUR CLIENT ID>\",\n","            \"scope\": [\"https://analysis.windows.net/powerbi/api/.default\"],\n","            \"secret\": \"<YOUR CLIENT SECRET>\",\n","            \"endpoint\": \"https://api.powerbi.com/v1.0/myorg/admin/activityevents\"\n","        }\n","\n","app = msal.ConfidentialClientApplication(\n","    config[\"client_id\"], authority=config[\"authority\"],\n","    client_credential=config[\"secret\"],\n","    )\n","result = None\n","result = app.acquire_token_silent(config[\"scope\"], account=None)\n","\n","if not result:\n","    logging.info(\"No suitable token exists in cache. Get a new one from AAD.\")\n","    result = app.acquire_token_for_client(scopes=config[\"scope\"])\n","\n","if \"access_token\" in result:\n","    # Calling graph using the access token\n","   logging.info(\"Got the Token\")\n","   print(\"Got the Token\")\n","else:\n","    print(result.get(\"error\"))\n","    print(result.get(\"error_description\"))\n","    print(result.get(\"correlation_id\"))"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"c09d0a61-26b7-418b-b03b-ea4bfa8a9d37"},{"cell_type":"code","source":["# Specify a start & end date manually, or retreive the last day of records. You cannot currently retreive audit logs older than 30 days, so retreive your logs often.\n","# start_date = date(2024, 8, 26)\n","# end_date = date(2024, 8, 26)\n","start_date = date.today() - timedelta(days=1)\n","end_date = date.today() - timedelta(days=1)\n","\n","delta = timedelta(days=1)\n","while start_date <= end_date:\n","    activityDate = start_date.strftime(\"%Y-%m-%d\")\n","\n","    url = \"https://api.powerbi.com/v1.0/myorg/admin/activityevents?startDateTime='\" + activityDate + \"T00:00:00'&endDateTime='\" + activityDate + \"T23:59:59'\"\n","\n","    access_token = result['access_token']\n","    header = {'Content-Type':'application/json', 'Authorization':f'Bearer {access_token}'}\n","    response = requests.get(url=url, headers=header)\n","\n","    response_obj = response.json()\n","    event_entities = response_obj[\"activityEventEntities\"]\n","    continuation_uri = response_obj[\"continuationUri\"]\n","    continuation_token = response_obj[\"continuationToken\"]\n","    activity_events = event_entities\n","    cont_count = 1\n","    while continuation_token is not None:\n","        response = requests.get(continuation_uri, headers=header)\n","        response_obj = response.json()\n","        event_entities = response_obj[\"activityEventEntities\"]\n","        continuation_uri = response_obj[\"continuationUri\"]\n","        continuation_token = response_obj[\"continuationToken\"]\n","\n","        activity_events.extend(event_entities)\n","        cont_count += 1\n","\n","    print(f\"Took {cont_count} tries to exhaust continuation token for {len(activity_events)} events.\")\n","\n","    df = pd.DataFrame(activity_events)\n","    Object_cols =[col for col, col_type in df.dtypes.items() if col_type==\"object\"]\n","    df[Object_cols] = df[Object_cols].astype(str)\n","\n","    float64_cols =[col for col, col_type in df.dtypes.items() if col_type==\"float64\"]\n","    df[float64_cols] = df[float64_cols].astype(str)\n","\n","    sdf = spark.createDataFrame(df)\n","\n","    # Save the audit logs to a lakehouse as Parquet files\n","    targetPath = f\"abfss://{workspace_id}@onelake.dfs.fabric.microsoft.com/{lakehouse_bronze_id}/Files/activity_events/in\"\n","\n","    if not notebookutils.fs.exists(targetPath):\n","        notebookutils.fs.mkdirs(targetPath)\n","    sdf = sdf.withColumn('Year', year(col(\"CreationTime\")))\n","    sdf = sdf.withColumn('Month', month(col(\"CreationTime\")))\n","    sdf = sdf.withColumn('Day', dayofmonth(col(\"CreationTime\")))\n","    sdf.write.mode(\"append\").option(\"mergeSchema\",\"true\").format(\"parquet\").partitionBy(\"Year\",\"Month\",\"Day\").save(targetPath)\n","    print(f\"Output files for {activityDate}\")\n","    start_date += delta"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"8a39d321-bce3-441e-86d8-712a3a56b049"},{"cell_type":"markdown","source":["### Use Metadata Scanning to build an inventory of all content in your Fabric/PBI tenant\n","https://learn.microsoft.com/en-us/fabric/governance/metadata-scanning-overview"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"30142443-bade-45f4-8b05-04ef4ebbd055"},{"cell_type":"code","source":["# Authentication \n","\n"," \n","import json, requests, pandas as pd\n","from datetime import datetime\n","import time\n","\n","tenant = \"<YOUR TENANT ID>\"\n","client = \"<YOUR CLIENT ID>\"\n","client_secret = \"<YOUR CLIENT SECRET>\"\n","\n","# Mount our bronze lakehouse to work with non-Spark APIs.\n","workspace_id = spark.conf.get(\"trident.workspace.id\")\n","lakehouse_bronze_id = notebookutils.lakehouse.get(\"<YOUR LAKEHOUSE NAME>\", workspace_id)['id']\n","base_path = f\"abfss://{workspace_id}@onelake.dfs.fabric.microsoft.com/{lakehouse_bronze_id}\"\n","mount_name = \"/bronze\"\n","mssparkutils.fs.mount(base_path, mount_name)\n","mount_points = notebookutils.fs.mounts()\n","local_path = \"file:\" + next((mp[\"localPath\"] for mp in mount_points if mp[\"mountPoint\"] == mount_name), None)\n"," \n","try:\n","    from azure.identity import ClientSecretCredential\n","except Exception:\n","     !pip install azure.identity\n","     from azure.identity import ClientSecretCredential\n"," \n","# Generates the access token for the Service Principal\n","api = 'https://analysis.windows.net/powerbi/api/.default'\n","auth = ClientSecretCredential(authority = 'https://login.microsoftonline.com/',\n","                              tenant_id = tenant,\n","                              client_id = client,\n","                              client_secret = client_secret)\n","access_token = auth.get_token(api)\n","access_token = access_token.token\n"," \n","print('\\nSuccessfully authenticated.')   \n","# print(local_path)   "],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"4399dda9-0de0-4390-b8c5-cd14c033b308"},{"cell_type":"code","source":["headers = {\"Content-Type\": \"application/json\", \"Authorization\": \"Bearer \" + access_token}\n","\n","# Base URL for Power BI API calls\n","base_url = \"https://api.powerbi.com/v1.0/myorg/admin\"\n","\n","# Get list of workspace IDs\n","workspaces_url = base_url + \"/workspaces/modified?excludePersonalWorkspaces=False&excludeInActiveWorkspaces=False\"\n","response = requests.get(workspaces_url, headers=headers)\n","\n","workspace_ids = [workspace[\"id\"] for workspace in response.json()]\n","\n","# Process workspaces in chunks of 25\n","chunk_size = 25\n","for i in range(0, len(workspace_ids), chunk_size):\n","    chunk = workspace_ids[i:i + chunk_size]\n","    \n","    # Trigger a scan for these workspaces\n","    scan_url = base_url + \"/workspaces/getInfo?lineage=true&datasourceDetails=true&datasetSchema=true&datasetExpressions=true&getArtifactUsers=true\"\n","    response = requests.post(scan_url, headers=headers, data=json.dumps({\"workspaces\": chunk}))\n","    \n","    if response.status_code == 202:\n","        scan_id = response.headers[\"location\"].split(\"/\")[-1]\n","        \n","        # Poll until the scan succeeds\n","        status_url = base_url + \"/workspaces/scanStatus/\" + scan_id\n","        while True:\n","            status_response = requests.get(status_url, headers=headers)\n","            status = status_response.json().get(\"status\", \"\")\n","            \n","            if status == \"Succeeded\":\n","                break\n","            elif status == \"Failed\":\n","                raise Exception(\"Scan Failed\")\n","                \n","            time.sleep(3)  # Wait for 3 seconds before polling again\n","        \n","        # Once scan succeeds, fetch scan results\n","        result_url = base_url + \"/workspaces/scanResult/\" + scan_id\n","        result_response = requests.get(result_url, headers=headers)\n","        \n","        # Write the results to JSON\n","        targetPath = local_path + \"/Files/workspaces/in\"\n","\n","        # Ensure the target directory exists\n","        if not notebookutils.fs.exists(targetPath):\n","            notebookutils.fs.mkdirs(targetPath) \n","\n","        now = datetime.today().strftime('%Y-%m-%d-%H-%M-%S')\n","\n","        targetFile = f\"{targetPath}/scan_results_{i//chunk_size}_{now}.json\"\n","\n","        # Serialize the JSON data\n","        json_data = json.dumps(result_response.json())\n","\n","        # Write the data to the Lakehouse using mssparkutils.fs.put\n","        mssparkutils.fs.put(targetFile, json_data, overwrite=True)\n","        print(f'Successfully written to {targetFile}')\n","\n","        # with open(f\"{targetPath}/scan_results_{i//chunk_size}_{now}.json\", \"w\") as f:\n","        #     json.dump(result_response.json(), f)\n","        #     print(f'Successfully written to {targetPath}/scan_results_{i//chunk_size}_{now}.json')"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"71094602-1f79-4798-9a04-882457e5881e"}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"name":"synapse_pyspark","display_name":"synapse_pyspark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{}},"nbformat":4,"nbformat_minor":5}